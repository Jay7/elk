###<----------------- INPUTS ----------------->###
input {
  pipe {
     type => "sflow"
     command => "sh /var/scripts/sflowtool-wrapper.sh -l -p 6343"
  }
}


#################
# Sflow Filters #
#################

filter {

    if [type] == "sflow" {

# sFlow sends two kinds of messages - CNTRs and FLOWs
# I'm not doing anything with CNTRs at this point, so
# I drop those, and we concentrate on processing FLOW
# messages.

        #if [message] =~ /CNTR/ {
        #    drop { }
        #}

# sFlow FLOW messages break down into the following fields.
# I have named them arbitrarily, but they correspond to the
# actual field names.  I developed this grok pattern using 
# two tools:
#
# - sflowtool (http://www.inmon.com/technology/sflowTools.php)
#   Written by InMon, it provides a way to examine sFlow messages
#   in human readable format.
#
# - Grok Debugger (https://grokdebug.herokuapp.com/)
#   Invaluable, and self-explanatory.

        grok {
            match => [ "message", "%{WORD:SampleType},%{IP:sflow.ReporterIP},%{NUMBER:sflow.inputPort},%{NUMBER:sflow.outputPort},%{GREEDYDATA:sflow.srcMAC},%{GREEDYDATA:sflow.dstMAC},%{WORD:sflow.EtherType},%{NUMBER:sflow.src_vlan},%{NUMBER:sflow.dst_vlan},%{IP:sflow.srcIP},%{IP:sflow.dstIP},%{NUMBER:sflow.IP_Protocol},%{WORD:sflow.IPTOS},%{NUMBER:sflow.IPTTL},%{NUMBER:sflow.srcPort},%{NUMBER:sflow.dstPort},%{DATA:sflow.tcpFlags},%{NUMBER:sflow.PacketSize},%{NUMBER:sflow.IPSize},%{NUMBER:sflow.SampleRate}"]
            match => [ "message", "%{WORD:Sample_Type},%{IP:Flow_Reporter},%{NUMBER:int_ifIndex},%{NUMBER:int_ifType},%{NUMBER:hyper_ifSpeed},%{NUMBER:int_ifDirection},%{NUMBER:int_ifStatus},%{NUMBER:hyper_ifInOctets},%{NUMBER:int_ifInUcastPkts},%{NUMBER:int_ifInMulticastPkts},%{NUMBER:int_ifInBroadcastPkts},%{NUMBER:int_ifInDiscards},%{NUMBER:int_ifInErrors},%{NUMBER:int_ifInUnknownProtos},%{NUMBER:hyper_ifOutOctets},%{NUMBER:int_ifOutUcastPkts},%{NUMBER:int_ifOutMulticastPkts},%{NUMBER:int_ifOutBroadcastPkts},%{NUMBER:int_ifOutDiscards},%{NUMBER:int_ifOutErrors},%{NUMBER:int_ifPromiscuousMode}"]  
       }

# Somet3imes it doesn't work out.

        #if "_grokparsefailure" in [tags] {
        #    drop { }
        #}

# Because I'll ultimately be displaying all of this
# in Kibana, I want to translate many of the IP addresses
# into recognizable hostnames.  We take the IP fields,
# and copy them into new fields that we'll be doing DNS
# lookups on:

        mutate {
            add_field => {
                "[sflow.srcHostname]" => "%{sflow.srcIP}"
                "[sflow.dstHostname]" => "%{sflow.dstIP}"
                "[sflow.ReporterName]" => "%{sflow.ReporterIP}"
            }
        }

# I also want to translate things like Source and
# Destination port numbers into known service names.
# for this to work, you have to built some YAML files,
# which basically map everything you'd find in an
# /etc/services file.  I'll post my YAML files in
# other Gists.

        translate {
            field => "[sflow.srcPort]"
            destination => "[sflow.srcPortName]"
            dictionary_path => "/etc/logstash/dictionaries/iana_services.yaml"
        }

        translate {
            field => "[sflow.dstPort]"
            destination => "[sflow.dstPortName]"
            dictionary_path => "/etc/logstash/dictionaries/iana_services.yaml"
        }

        translate {
            field => "[sflow.IP_Protocol]"
            destination => "[sflow.IP_ProtoName]"
            dictionary_path => "/etc/logstash/dictionaries/iana_protocols.yaml"
        }

        translate {
            field => "[sflow.tcpFlags]"
            destination => "[sflow.tcpFlagDecode]"
            dictionary_path => "/etc/logstash/dictionaries/tcp_flags.yaml"
        }

# Here we do our DNS reverse lookups:

        dns {
            reverse => [ "sflow.srcHostname" ]
            action => "replace"
        }

        dns {
            reverse => [ "sflow.dstHostname" ]
            action => "replace"
        }

        dns {
            reverse => [ "sflow.ReporterName" ]
            action => "replace"
        }

      geoip {
            source => "[sflow.srcIP]"
            target => "sflow.src_geoip"
            add_field => [ "[sflow.src_geoip][coordinates]", "%{[sflow.src_geoip][longitude]}" ]
            add_field => [ "[sflow.src_geoip][coordinates]", "%{[sflow.src_geoip][latitude]}"  ]
            add_tag => [ "sflow" ]
      }
      geoip {
            source => "[sflow.dstIP]"
            target => "sflow.dst_geoip"
            add_field => [ "[sflow.dst_geoip][coordinates]", "%{[sflow.dst_geoip][longitude]}" ]
            add_field => [ "[sflow.dst_geoip][coordinates]", "%{[sflow.dst_geoip][latitude]}"  ]
            add_tag => [ "sflow" ]
      }


# If the reverse lookup didn't fail (leaving us with
# nothing but numerics) we grok the FQDN, parsing it
# into a HOST and DOMAIN - and then we store the
# HOST portion into a new field - sflow.srcHost
#
# Otherwise, we just dump the unresolved IP address
# into the sflow.srcHostname field.

        if [sflow.srcHostname] !~ /(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)/ {
            grok {
                match => [ "[sflow.srcHostname]", "%{DATA:src_host}\.%{GREEDYDATA:domain}" ]
                add_field => { "[sflow.srcHost]" => "%{src_host}" }
            }
        } else {
            mutate {
                add_field => [ "[sflow.srcHost]", "%{[sflow.srcHostname}" ]
            }
        }

        if [sflow.DstHostname] !~ /(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)/ {
            grok {
                match => [ "[sflow.dstHostname]", "%{DATA:dst_host}\.%{GREEDYDATA:domain}" ]
                add_field => { "[sflow.dstHost]" => "%{dst_host}" }
            }
        } else {
            mutate {
                add_field => [ "[sflow.dstHost]", "%{[sflow.dstHostname}" ]
            }
        }

        if [sflow.ReporterName] !~ /(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)/ {
            grok {
                match => [ "[sflow.ReporterName]", "%{DATA:rep_host}(\.|\-)" ]
                add_field => { "[sflow.Reporter" => "%{rep_host}" }
            }
        } else {
            mutate {
                add_field => [ "[sflow.Reporter]", "%{[sflow.ReporterName}" ]
            }
        }
 

# There are a bunch of fields from the original
# message, as well as some I created temporarily,
# that I don't care to store.  Here's where I drop
# them all.

        #mutate {
        #    remove_field => [ "host", "command", "sflow.inputPort", "sflow.outputPort", "sflow.srcMAC", "sflow.dstMAC", "sflow.EtherType", "sflow.in_vlan", "sflow.out_vlan", "sflow.IPTTL", "sflow.IPSize", "message", "src_host", "dst_host", "rep_host", "SampleType", "domain" ]
        #}

# Lastly, we're going to want Kibana and
# Elasticsearch to be able to do some math
# with certain fields, so make sure they're
# handled as numbers, rather than strings.
# You can also do this by setting up templates
# in Elasticsearch, but this is easier.

        #mutate {
        #        convert => {
        #                "sflow.PacketSize" => "integer"
        #                "sflow.SampleRate" => "integer"
        #        }
        #}
        mutate {
                convert => [ "sflow.PacketSize", "integer" ]
                convert => [ "sflow.SampleRate", "integer" ]
                convert => [ "IPSize", "integer" ]
                #convert => [ "FW_geoip.area_code", "integer" ]
                #convert => [ "FW_geoip.dma_code", "integer" ]
                convert => [ "sflow.src_geoip.latitude", "float" ]
                convert => [ "sflow.src_geoip.longitude", "float" ]
                convert => [ "sflow.dst_geoip.latitude", "float" ]
                convert => [ "sflow.dst_geoip.longitude", "float" ]
                convert => [ "sflow.srcPort", "integer" ]
                convert => [ "sflow.dstPort", "integer" ]

                convert => [ "hyper_ifSpeed", "integer" ]
                convert => [ "hyper_ifInOctets", "integer" ]
                convert => [ "int_ifInUcastPkts", "integer" ]
                convert => [ "int_ifInMulticastPkts", "integer" ]
                convert => [ "int_ifInBroadcastPkts", "integer" ]
                convert => [ "int_ifInDiscards", "integer" ]
                convert => [ "int_ifInErrors", "integer" ]
                convert => [ "hyper_ifOutOctets", "integer" ]
                convert => [ "int_ifOutUcastPkts", "integer" ]
                convert => [ "int_ifOutMulticastPkts", "integer" ]
                convert => [ "int_ifOutBroadcastPkts", "integer" ]
                convert => [ "int_ifOutDiscards", "integer" ]
                convert => [ "int_ifOutErrors", "integer" ]
              }

    }
}

###<----------------- OUTPUT ----------------->###
output {
    stdout { codec => rubydebug }
    elasticsearch {
      protocol => "http"
      node_name => "sflow-logstash"
      cluster => "elk-cluster-docker"
      host => elasticsearch
    }
}
